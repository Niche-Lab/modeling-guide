# Abstract

This review critically examines the metrics and methodologies used for evaluating prediction models in regression and classification tasks, underscoring the importance of rigorous and standardized approaches in model performance assessment. Modeling, a structured framework for hypothesis formulation and decision-making, relies on the analysis and extrapolation of empirical data. Its advancement is contingent on the accumulation and integrity of prior knowledge within the scientific community.

The review highlights that no single metric suffices to fully grasp model performance, emphasizing the need for understanding the underlying theory of each metric to avoid misleading conclusions. In regression tasks, metrics like the Correlation Coefficient r, RMSE, and R^2 are discussed, considering their specific applications and limitations. For classification tasks, the focus shifts to metrics like precision, recall, ROC curve, and MCC curve, with an emphasis on correctly designating the positive class to prevent bias and ensuring label-invariant assessment for a balanced evaluation. Moreover, the paper delves into common pitfalls in cross-validation (CV), a technique crucial for simulating unseen data for model evaluation. Issues such as using the same data for both training and assessment, excluding model selection from CV, and overlooking experimental block effects are explored. These pitfalls are demonstrated through hypothetical examples, complemented by real-world cases from literature.

The review is structured into sections covering model performance metrics and validation techniques. Discussions include the impact of bias and variance, model selection methodologies, and the implementation of block cross-validation strategies. Each section provides computational simulations and literature examples to illustrate the concepts from both theoretical and practical perspectives, culminating in a summary of key points and recommendations for future research. This comprehensive approach aims to guide researchers in accurately reporting model performance, especially in scenarios of imbalanced datasets, feature selection, hyperparameter tuning, and limited sample sizes, ultimately contributing to the integrity and progress of scientific research in modeling.
