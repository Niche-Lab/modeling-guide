\section*{Appendix}

\subsection*{Cross Validation}

Model cross validation aims to evaluate how well a given model generalizes to an independent dataset that it has not seen during the training process. The most common method is K-fold cross-validation (\textbf{K-fold CV}). To implement the K-fold CV, the available dataset, denoted as $\mathcal{D}$, is partitioned into K equally sized folds. We can express the dataset as below:


\begin{equation} \label{eq_datasplit}
\begin{split}
    \mathcal{D} & = \{(X, Y)\} \\
    & = \{(X_1, Y_1), (X_2, Y_2), \dots, (X_K, Y_K)\}
\end{split}
\end{equation}

where $X\in\mathbb{R}^{n\times p}$ represents the input features, and $Y\in\mathbb{R}^{n\times1}$ symbolizes the ground truth labels for a single target variable. The value of n\ corresponds to the total number of samples, while p represents the number of features. In each iteration of the K-fold CV, a single fold is reserved as the test set, $\mathcal{D}_{\mathrm{test}}$ (or $\mathcal{D}_k$), to act as unseen data, while the remaining folds make up the training set $\mathcal{D}_{\mathrm{train}}$ (or $\mathcal{D}_\text{-k}$):

\begin{equation} \label{eq_traintest}
\begin{split}
    \mathcal{D}_{\text{train}} &= \mathcal{D}_{\text{-k}} \\
    &= \{(X_1, Y_1), (X_2, Y_2), \dots, (X_{k-1}, Y_{k-1}), (X_{k+1}, Y_{k+1}), \dots, (X_K, Y_K)\}\\
    \mathcal{D}_{\text{test}} &= \mathcal{D}_{k} \\
    &= \{(X_k, Y_k)\}
\end{split}
\end{equation}

After splitting the dataset into $\mathcal{D}_\text{-k}$ and $\mathcal{D}_k$, the examined model $f$ is trained on the training set $\mathcal{D}_\text{-k}$ and denoted as $f_{\mathcal{D}_{\text{-k}}}$. The hold-out test set $\mathcal{D}_k$ is then used to evaluate the model performance $\hat{g}\left(f_{\mathcal{D}_{\text{-k}}}\right)$, which is defined by comparing the predicted labels $\hat{Y}_{k} = f_{\mathcal{D}_{\text{-k}}}(X_k)$ with the true labels $Y_k$ using a performance metric $\mathcal{L}$ (e.g., RMSE or $R^2$):

\begin{equation} \label{eq_g_est}
    \begin{split}
\hat{g}(f_{\mathcal{D}_{\text{-k}}}) &= \mathcal{L}(Y_k, \hat{Y_k}) \\
    &= \mathcal{L}(Y_k, f_{\mathcal{D}_{\text{-k}}}(X_k))
    \end{split}
\end{equation}

To estimate the generalization performance of a model $\mathbb{E}[\hat{g}(f_\mathcal{D})]$, the K-fold CV procedure is repeated K times until each fold has been used as the test set $\mathcal{D}_k$ once. The entire dataset $\mathcal{D}$ is leveraged to calculate the average prediction performance over all K folds. The model's generalization performance can be expressed as:

\begin{equation} \label{eq_g_exp}
    \begin{split}
        \mathbb{E}[\hat{g}(f_{\mathcal{D}})] &= \mathbb{E}[\hat{g}(f_{\mathcal{D}_{\text{-k}}})] \\
        &= \frac{1}{K}\sum_{k=1}^{K} \hat{g}(f_{\mathcal{D}_{\text{-k}}})
    \end{split}
\end{equation}

It is noted that $\mathbb{E}[\hat{g}(f_\mathcal{D})]$ is equivalent to $\mathbb{E}[\hat{g}(f_{\mathcal{D}_{\text{-k}}})]$ in K-fold CV. It is because the $\mathbb{E}[\hat{g}(f_\mathcal{D})]$ is estimated by averaging all $\hat{g}(f_{\mathcal{D}_{\text{-k}}}) $ over K folds, which is also the definition of $\mathbb{E}[\hat{g}(f_{\mathcal{D}_{\text{-k}}})]$.

\subsection*{Cross Validation Bias and Variance}

The true generalization performance of the model $G(f_\mathcal{D})$ can only be approximated by averaging the performance metrics over infinite unseen datasets. However, in practice, the dataset $\mathcal{D}$ is finite and therefore, there is always a bias when using a finite dataset to estimate $G(f_\mathcal{D})$. The bias is known as validation bias:

\begin{equation} \label{eq_bias}
    \mathrm{Bias}=\mathbb{E}[\hat{g}\ (f_\mathcal{D})]-G(f_{D})
\end{equation}

For example, if RMSE is used as the performance metric, a positive validation bias suggests that the model validation procedure concludes a pessimistic estimation of the model performance, since the true performance is expected to be lower than the estimated performance.
Another aspect of model validation is the variance of the estimated performance. For example, in a 5-fold cross-validation, there are five estimates of the model performance. The variance among these five estimates is known as validation variance. A high validation variance suggests that the performance is sensitive to the choice of the test set $\mathcal{D}_k$, which may be caused by a small sample size or an over-complex model. The validation variance can be defined as:

\begin{equation} \label{eq_var}
    \begin{split}
        \mathrm{Variance}&=\mathbb{E}[(\hat{g}(f_{\mathcal{D}_{\text{-k}}})-\mathbb{E}[\hat{g}(f_\mathcal{D})])^{2}]\\
        &=\mathbb{E}[{\hat{g}}^2(f_{\mathcal{D}_{\text{-k}}}) - 2\hat{g}(f_{\mathcal{D}_{\text{-k}}})\mathbb{E}[\hat{g}(f_\mathcal{D})] + \mathbb{E}^{2}[\hat{g}(f_{\mathcal{D}})]]\\
        &=\mathbb{E}[{\hat{g}}^2(f_{\mathcal{D}_{\text{-k}}})] - 2\mathbb{E}[\hat{g}(f_{\mathcal{D}_{\text{-k}}})]\mathbb{E}[\hat{g}(f_{\mathcal{D}})] + \mathbb{E}^{2}[\hat{g}(f_{\mathcal{D}})]\\
        &=\mathbb{E}[{\hat{g}}^2(f_{\mathcal{D}_{\text{-k}}})] - \mathbb{E}^{2}[\hat{g}(f_{\mathcal{D}})]
    \end{split}
\end{equation}

Combining the Equations~\ref{eq_bias} and~\ref{eq_var}, the mean squared error (MSE) of the model validation can be decomposed as:

\begin{equation} \label{eq_tradeoff}
    \begin{split}
        \mathrm{MSE}&=\mathbb{E}[(\hat{g}(f_{\mathcal{D}_{\text{-k}}})-G(f_\mathcal{D}))^2]\\
        &=\mathbb{E}[{\hat{g}}^2(f_{\mathcal{D}_{\text{-k}}})] - 2\mathbb{E}[g(f_{D_\text{-k}})]G(f_\mathcal{D})+G^2(f_\mathcal{D}) +\\
        &\quad \; \mathbb{E}^2[\hat{g}(f_{\mathcal{D}_{\text{-k}}})] - \mathbb{E}^2[\hat{g}(f_{D_\text{-k}})]\\
        &=(\mathbb{E}^2[\hat{g}(f_{\mathcal{D}_{\text{-k}}})] - 2\mathbb{E}[\hat{g}(f_{D_\text{-k}})]G(f_{\mathcal{D}}) + G^{2}(f_{\mathcal{D}})) +\\
        &\quad \; (\mathbb{E}[{\hat{g}}^2(f_{\mathcal{D}_{\text{-k}}})]-\mathbb{E}^2[\hat{g}(f_{\mathcal{D}_{\text{-k}}})])\\
        &=(\mathbb{E}[\hat{g}(f_{\mathcal{D}_{\text{-k}}})]-G(f_{\mathcal{D}}))2+(\mathbb{E}[g2(f_{\mathcal{D}_{\text{-k}}})]-E^2[\hat{g}(f_{\mathcal{D}_{\text{-k}}})])\\
        &=(\mathbb{E}[\hat{g}(f_\mathcal{D})] - G(f_{\mathcal{D}}))^{2} +(\mathbb{E}[\hat{g}^2(f_{\mathcal{D}_{\text{-k}}})]-\mathbb{E}^2[\hat{g}(f_{\mathcal{D}})])\\
        &={\mathrm{Bias}}^2+\mathrm{Variance}
    \end{split}
\end{equation}

\subsection*{Hyperparameter}

Here are the loss functions for ordinary least squares (OLS), ridge regression, and LASSO regression, respectively:


\begin{equation} \label{eq_ols}
    \mathcal{L}_\text{OLS}(\beta)=\sum_{i=1}^{n}(y_i-x_i\beta)^2
\end{equation}

\begin{equation} \label{eq_ridge}
    \mathcal{L}_\text{ridge}(\beta)=\sum_{i=1}^{n}(y_i-x_i\beta)^2+\lambda\sum_{j=1}^{p}\beta_{j}^2
\end{equation}

\begin{equation} \label{eq_lasso}
    \mathcal{L}_\text{LASSO}(\beta)=\sum_{i=1}^{n}(y_i-x_i\beta)^2+\lambda\sum_{j=1}^{p}|\beta_j|
\end{equation}

Where $x_i$ and $y_i$ represent the ith row of the design matrix $X$ and the response vector Y, respectively. The term n denotes the sample size, and $\beta$ is the coefficient vector. All three models aim to find the optimal $\beta$ that minimizes their respective loss function, $\mathcal{L}$. In the regularized models (i.e., ridge and LASSO regression), the vector length of $\beta$ is penalized in the loss function.

\subsection*{Sqaured Correlation Coefficient $r^2$ and Determination Coefficient $R^2$}

The squared Pearson correlation coefficient, \( r^2 \), is not necessarily equivalent to the coefficient of determination, \( R^2 \). This equivalence holds true specifically in the context of least squares regression when the same model and data are used for both fitting and evaluation. However, this may not be the case when the model is assessed using new data. To demonstrate the equivalence between \( r^2 \) and \( R^2 \) under these specific conditions, we begin by assuming that the covariance between the predicted values \(\hat{Y}\) and the residuals \(\epsilon\) is zero:


\begin{equation} \label{eq_pf_cov}
    \begin{split}
        \text{cov}(Y, \hat{Y}) &= \text{cov}(\hat{Y} + \epsilon, \hat{Y}) \\
        &= \text{cov}(\hat{Y}, \hat{Y}) + \text{cov}(\hat{Y}, \epsilon) \\
        &= \text{var}(\hat{Y}) + \text{cov}(\hat{Y}, \epsilon) \\
        &= \text{var}(\hat{Y}) \\
    \end{split}
\end{equation}

With the assumption that \(\bar{\hat{Y}} = \bar{Y}\), which typically holds when \(\mathbb{E}[\epsilon] = 0\), the squared correlation coefficient \( r^2 \) is expressed as follows:

\begin{equation} \label{eq_pf_r2}
    \begin{split}
        r^2 &= \frac{\text{cov}^2(Y, \hat{Y})}{\text{var}(Y)\text{var}(\hat{Y})} \\
        &= \frac{\text{var}(\hat{Y})^2}{\text{var}(Y)\text{var}(\hat{Y})} \\
        &= \frac{\text{var}(\hat{Y})}{\text{var}(Y)} \\
        &= \frac{\sum\limits_{i=1}^{n}(\hat{Y}_i - \bar{\hat{Y}})^2}{\sum\limits_{i=1}^{n}(Y_i - \bar{Y})^2} \\
        &= \frac{\sum\limits_{i=1}^{n}(\hat{Y}_i - \bar{Y})^2}{\sum\limits_{i=1}^{n}(Y_i - \bar{Y})^2} \\
        &= \frac{SS_{\text{regression}}}{SS_{\text{total}}} \\
        &= R^2
    \end{split}
\end{equation}

where \(SS_{\text{regression}}\) is the variation explained by the model and \(SS_{\text{total}}\) is the total sum of squares. Each \(Y_i\) and \(\hat{Y}_i\) are the ith elements of the actual response vector \(Y\) and the predicted response vector \(\hat{Y}\), while \(\bar{Y}\) and \(\bar{\hat{Y}}\) are their respective means. This proof highlights that under certain assumptions, \( r^2 \) and \( R^2 \) can indeed be equivalent, but such conditions are specific to least squares regression where errors are normally distributed and predictions are unbiased estimates of the actual values.
