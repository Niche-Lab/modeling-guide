# Summary

## Interpretive Summary

This simulation study provides practical guidelines to support evaluating prediction models used in precision agricultural contexts. It emphasizes common pitfalls in cross-validation, a key method for model assessment. Additionally, the study addresses the need for a deep understanding of various performance metrics, highlighting their applications and limitations. By offering computational simulations and practical examples, this study is intended to support the decision-making process of researchers developing prediction models, supporting transparent and accurate reporting of model performance. The guidelines are intended to support integrity and scientific rigor in prediction modeling research.

## Abstract

This study critically examines the methodologies and metrics used for evaluating prediction models in regression and classification tasks, making a case for the application of rigorous and standardized approaches in model performance assessment. Within the context of this work, we define modeling as a structured framework for hypothesis formulation and decision-making, which relies on the analysis and extrapolation of empirical data. The advancement of modeling is contingent on the accumulation of prior knowledge within the scientific community.

The study conducted a series of simulations to delve into common pitfalls in cross-validation (CV), a technique crucial for characterizing expected model performance on “new” data.  Issues such as using the same data for both training and assessment, excluding model selection from CV, and overlooking experimental block effects were explored through simulation examples. Moreover, the simulations in this study highlight that no single model performance metric suffices to represent model performance adequately and conservatively, emphasizing the need for understanding the underlying theory of each metric to avoid misleading conclusions. In conclusion, this simulation study aims to guide researchers in accurately and consistently reporting model performance, thereby supporting integrity and scientific rigor in prediction modeling research.
