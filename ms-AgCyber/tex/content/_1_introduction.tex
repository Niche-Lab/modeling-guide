\section{Introduction}

\subsection{Modeling}

Modeling is an essential tool for hypothesis formulation and decision-making. It functions as a structured investigatory framework that allows researchers to explore system understanding through the summary and analysis of empirical data. Carefully constructed and evaluated models offer the potential to extend this understanding by enabling the extrapolation of results to novel trials and conditions. Although only one focus of the science of modeling, the predictive role is often explicitly or implicitly the ultimate goal of models derived within the precision agriculture context. Through this lens, modeling provides opportunity to standardize and formalize research advancement, through developing quantitative constructs that accumulate prior knowledge derived by the broader the scientific community. Evaluating model performance becomes particularly critical when considering this role within the knowledge generation enterprise, necessitating a rigorous and standardized approach that allows for both reproducibility and comparability. As more and more model-based exercises are developed using slightly different methods, or slightly different datasets, it becomes increasingly challenging to evaluate, characterize, compare, and balance information generated by the resulting modeling tools, particularly when results are conflicting. Specifically, reporting model performance through poorly-defined metrics or incomplete procedures can create opportunity for confusion, misinterpretation, and miscommunication, while can ultimately result in distrust in model-based tools and impede scientific progress.

Here, we review two types of challenges that can be encountered during the model evaluation process: challenges in data structure and challenges in evaluation approach. Data structure challenges include those inherent to the types of data used in a modeling exercise. For continuous data types, challenges include measurement variance, extreme observations, and underlying variation structures like blocks. For categorical data, challenges largely center around the balance or lack thereof between categories.  Challenges in the evaluation approach are driven by decision-making about which data are used for model derivation and which are used for evaluation. We will review these challenges in this study.

\subsection{Model Evaluation}

Model evaluation in the context of predictive analytics seeks to explore how well a model can generalize to new prediction contexts not seen during model training. Although commonly referred to as "model validation" in the literature, this term implies a false degree of confidence given that the word “validation” means to prove something true. There is no single test, or recognized suite of tests, to prove a model valid. Instead, the term "evaluation," which involves assessing the value, nature, character, or quality of something, is more fitting. It is essential to evaluate a model performance on unseen data to ensure the approach is applicable to new experiments. To this end, cross-validation (CV) is widely recognized as a standard method for model evaluation.

The most common CV method is K-fold CV, which partitions the dataset into K equally sized folds. In each iteration, one fold is reserved as the test set (i.e., new data, noted as $\mathcal{D}_{\text{test}}$), while the remaining folds are used as the training set (noted as $\mathcal{D}_{\text{train}}$) to construct the model. Once the model completes training, it is evaluated on the $\mathcal{D}_{\text{test}}$ to obtain a estimate of the model performance $\hat{g}$. The process will iterate K times until each fold has been used as the $\mathcal{D}_{\text{test}}$ once. And the average performance over all K folds is deemed as the expected generalization performance of the model $\mathbb{E}[\hat{g}]$ on new data.

However, there is always a evaluation bias between the estimated performance $\mathbb{E}[\hat{g}]$ and the true generalization performance $G$, which can only be approximated by evaluating the same model on an infinite number of unseen data. If RMSE is used as the performance metric, which is lower the better for the model performance, a positive evaluation bias $\mathbb{E}[\hat{g}] - G$ suggests that the model evaluation procedure concludes a pessimistic estimation of the model performance, since the true performance is expected to be lower than the estimated performance. Another aspect of model evaluation error is the variance of each estimated performance $\hat{g}$ across the K folds. For example, there are five estimates in a 5-fold cross-validation. The variance among these five estimates is defined as the evaluation variance. A high evaluation variance suggests that the performance is sensitive to the choice of the test set $\mathcal{D}_\text{test}$, which may be caused by a small sample size or an over-complex model.

There is a trade-off relationship between the evaluation bias and variance from a squared evaluation bias, the derivation of the relatinoship is shown in the Eq. ~\ref{eq_tradeoff} in the Appendix. When performing K-fold CV with a fixed sample size and model complexity, the choice of K is the pivotal element shaping the model evaluation. When the K is set to a larger value; each training set $\mathcal{D}_\text{train}$ is larger in size, resulting in a model trained on a more representative subset of the population of interest, leading to lower bias. However, a large K comes with a trade-off: the corresponding test subset $\mathcal{D}_\text{test}$ is compressed in size, making the tested model more sensitive to the specific data points, and thus inflating the validation variance. Conversely, a smaller K, along with a minor training set $\mathcal{D}_\text{train}$ , reduces their representativeness and increases bias. Nevertheless, a larger size of the test set $\mathcal{D}_\text{test}$ leads to more consistent estimations across the folds and, consequently, reduces the validation variance.

Leave-one-out cross-validation (LOOCV) is a variant of K-fold CV where K equals the sample size of the complete dataset $\mathcal{D}$. It provides an unbiased estimation of model performance because the training set $\mathcal{D}_\text{train}$ closely resembles the unseen population of interest, given its size of $N - 1$, where $N$ is the sample size. However, as the trade-off discussion suggested, this method can lead to high validation variance due to the model is evaluated on one sample at a time. The true unbiased nature of LOOCV is fully realized only when all K folds are utilized. Performing an incomplete LOOCV can introduce significant bias because of the inherent high validation variance, which often occurs when training each model iteration is prohibitively time-consuming or computationally demanding. In specific contexts, such as genomic prediction, strategies like the one described by Cheng et al. leverage the matrix inverse lemma, which allows for computational savings by avoiding the inversion of large matrices in each fold. This technique significantly reduces the dependency of computational resources on the sample size \citep{cheng_efficient_2017}. Van Dixhoorn et al. exemplify the use of LOOCV with a small dataset, aiming to predict cow resilience with limited data resources \citep{van_dixhoorn_indicators_2018}. Nevertheless, for large datasets, LOOCV is generally not recommended due to computational inefficiency. Further details of bias-variance trade-off have been extensively explored in the statistical literature \citep{hastie_elements_2009, cawley_over-fitting_2010}.

\subsection{Model Selection}

Model selection becomes necessary when models are not entirely determined by the data alone. For example, in a regularized linear regression model such as a ridge regression \citep{hoerl_ridge_1970} or the least absolute shrinkage and selection operator (LASSO) \citep{tibshirani_regression_1996},  it is essential to define a regularization parameter, $\lambda$, before fitting the model to the data. A larger $\lambda$ value yields a more regularized model, which tends to reduce smaller coefficients to negligible values or zero. This approach helps in preventing overfitting noise in the training data. The definition of loss functions for the regularized models were described in ~\ref{eq_ridge} and ~\ref{eq_lasso} of the Appendix.

These pre-defined parameters, which influence model fitting and remain constant during the training process, are known as hyperparameters. Beyond regularized models, hyperparameters are crucial in other predictive models, enhancing flexibility and robustness. For example, in the Support Vector Regression (SVR) \citep{drucker_support_1996}, the regressors X are projected onto a linear subspace to approximate the target variable Y. By choosing a suitable kernel function, which transforms the regressors into a non-linear space, as a hyperparameter, SVR can more effectively capture non-linear relationships, thus significantly improving model performance. Another hyperparameter example is the number of latent variables in the Partial Least Square (PLS) Regression \citep{abdi_partial_2003}, which condenses the original regressors into a more manageable set of latent variables, reducing multicollinearity issues. Fewer latent variables might lose significant information from the original regressors, while too many can lead to overfitting. Similarly, in Random Forest \citep{breiman_random_2001}, hyperparameters such as tree depth and the number of trees dictate model complexity. The same applies to the number of hidden layers and the size of filters in convolutional neural networks \citep{lecun_generalization_1989}. All these examples highlight the fact that selecting the most suitable hyperparameters, which is known as hyperparameter tuning, is crucial for optimizing model performance.
Feature selection is another crucial aspect of model selection. This process involves fitting the model to a selected subset of the original features, particularly essential in high-dimensional data scenarios where the number of features exceeds the number of observations, leading to poor model generalization. For instance, Ghaffari et al. sought to predict health traits in 38 multiparous Holstein cows using a metabolite profiling strategy. Out of 170 metabolites, only 12 were identified as effective discriminators between healthy and over-conditioned cows and were thus selected for the predictive model \citep{ghaffari_metabolomics_2019}. Therefore, optimizing feature subsets is a vital model selection strategy that significantly affects model performance.
Including the model selection process within the cross-validation is essential to avoid common pitfalls. The risk of inflated model performance arises when model selection is guided by results on the test dataset. Even if the chosen model is subjected to k-fold cross-validation afterward, its selection bias toward the test set can lead to overestimating its efficacy. This issue has been highlighted in statistical literature \citep{hastie_elements_2009}. A practical solution is to divide the dataset into training, validation, and test sets. The validation set is then used for model selection, ensuring the test set remains completely unused during the training phase, thereby providing a more accurate measure of model performance. For instance, the study by Rovere et al. exemplifies best practices in hyperparameter tuning and feature selection by employing an independent cross-validation step prior to assessing model performance. This approach enabled the precise selection of relevant spectral bands from the mid-infrared spectrum and the optimal number of latent dimensions in PLS with Bayesian regression for predicting the fatty acid profile in milk \citep{rovere_prediction_2021}. Similarly, Becker et al. demonstrated a robust evaluation by using nested cross-validation loops; the inner loop conducted a grid search for the best hyperparameters in logistic regression, while the outer loop was designed to evaluate the performance of the resulting optimized model \citep{becker_predicting_2021}. Both examples underscore the importance of separating model selection from performance evaluation to ensure the validity and reliability of the results.

\subsection{Cross Validation Design with Block Effects}

Blocking is an essential approach in experimental design to control for variations that can confound the variable of interest. For instance, Lahart et al. investigated the dry matter intake of grazing cows using mid-infrared (MIR) spectroscopy technology across multiple herds under varying experimental conditions \citep{lahart_predicting_2019}. Given the significant variation between herds, which may contribute to individual differences in both dry matter intake and MIR spectra, it is crucial to consider the herd as a blocking factor before evaluating the predictability of dry matter intake using MIR spectra. This consideration should also extend to model validation. In the cited study, variations in dry matter intake, the primary focus of the prediction model, were observed to exceed one standard deviation among some herds. In cross-validation, if samples from the same herd are assigned to different folds, with one fold used as the test set, the model is likely to achieve high accuracy. This accuracy may largely result from explaining the inter-herd variation rather than individual variations in dry matter intake, leading to an overestimation of model performance. To avoid this pitfall, block cross-validation, where each block (i.e., herd in this example) is used as a fold, is recommended for unbiased model validation.
Literature reviews have indicated that block cross-validation effectively evaluates model performance on external or unseen datasets \citep{bresolin_infrared_2020}. In the same study by Lahart et al., three cross-validation strategies were compared: random cross-validation (Random CV), which randomly assigns samples to folds; within-herd validation, training and testing the model within each herd; and across-herd validation (Block CV), where each herd is used as a fold and tested in turn. The results showed that performance estimates in block CV were noticeably lower than the other two strategies, supporting the hypothesis that ignoring block effects inflates model performance. Other studies considering block effects, including diet \citep{grelet_potential_2020}, herd \citep{rovere_prediction_2021}, and farm location \citep{adriaens_productive_2020, mota_real-time_2022}, have shown similar results in cross-validation, demonstrating block CV's effectiveness in evaluating model performance on external datasets.

\subsection{Model Performance Metrics}

Model performance metrics serve as quantitative indicators for evaluating model performance. They are critical for benchmarking various modeling approaches and for evaluating hypotheses underpinning these different approaches. Choosing appropriate metrics to support hypothesis testing is crucial, as in-ideal selection may lead to overly optimistic conclusions. Due to the different goals of regression and classification tasks, it is critical to ensure that these different model types are evaluated using different metrics. As such, metrics for regression and classification are discussed individually.

\subsubsection{Metrics in Regression Tasks}

\begin{table}
    \caption{Summary of model performance metrics for regression tasks.}
    \centering
    \begin{tabular}{llll}
        \toprule
        Metric & Type & Scale-invariant & Range \\
        \midrule
        Root mean square error (RMSE) & Error-Based & No & [0, $\infty$] \\
        Mean absolute error (MAE) & Error-Based & No & [0, $\infty$] \\
        Root mean squared percentage error (RMSPE) & Error-Based & Yes & [0, 1] \\
        Root mean standard deviation ratio (RSR) & Error-Based & Yes & [0, 1] \\
        Pearon's correlation coefficient (r) & Linearity-Based  & Yes & [-1, 1] \\
        Coefficient of determination (R$^2$) & Linearity-Based & Yes & [-$\infty$, 1] \\
        Lin's concordance correlation coefficient (CCC) & Linearity-Based & Yes & [-1, 1] \\
        \bottomrule
    \end{tabular}
    \label{tab:metrics-reg}
\end{table}

Regression models aim to predict continuous variables and are commonly employed in diverse applications, such as estimating body condition scores \citep{spoliansky_development_2016, yukun_automatic_2019}, body weight \citep{song_automated_2018,xavier_use_2022}, milk composition \citep{rovere_prediction_2021,mota_real-time_2022,mantysaari_body_2019,frizzarin_predicting_2021}, efficiency of feed resource usage \citep{grelet_potential_2020, appuhamy_prediction_2016,de_souza_predicting_2018}, and early-lactation behavior \citep{van_dixhoorn_indicators_2018}. The metrics in regression tasks evaluate the agreement between the predicted value $\hat{y}$ and the true values $y$. The agreement can be generally quantified in two ways: error-based metrics and linearity-based metrics. The metrics are summarized in Table~\ref{tab:metrics-reg}. Error-based metrics focus on the deviation of each pair of predicted and true values, while linearity-based metrics consider overall linear relationships between the predictions and the truths. The root mean square error (RMSE) and the mean absolute error (MAE) are two common error-based metrics:

\begin{equation} \label{eq_rmse}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}

\begin{equation} \label{eq_mae}
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

where $y_i$ and $\hat{y}_i$ are the true and predicted values, respectively, and $n$ is the sample size. Both metrics preserve the scale of the original data, making them easy to interpret in real-world units. Additionally, compared to MAE, RMSE penalizes large errors more due to the squared term, making it more sensitive to outliers. In the cow production, monitoring animal body weight is a common practice to aid in the management of dairy cows. Studies by Song et al. and Xavier et al. have utilized RMSE to assess the effectiveness of three-dimensional cameras in estimating dairy cow body weight, yielding RMSE values of 41.2 kg and 12.1 kg, respectively \citep{song_automated_2018,xavier_use_2022}. These figures provide a straightforward value for farmers to gauge whether the prediction error is tolerable, considering their specific operational costs and management thresholds. In essence, RMSE translates complex model accuracy into practical insights for productive agricultural units.
When evaluating the same model across different traits, which may have different scales, a common practice is to express error metrics in a scale-free manner. This can be achieved by expressing RMSE as a percent of the mean observed value, such as root mean squared percentage error (RMSPE), or as a Root Mean Standard Deviation Ratio (RSR) that normalizes the RMSE by the standard deviation of the observed values:

\begin{equation} \label{eq_rmspe}
    \text{RMSPE} = \frac{\text{RMSE}}{\bar{y}}
\end{equation}

\begin{equation} \label{eq_rsr}
    \text{RSR} = \frac{\text{RMSE}}{\sigma_y}
\end{equation}

where $\bar{y}$ and $\sigma_y$ are the mean and standard deviation of the observed values, respectively. When expressed as a percent, RMSPE typically ranges from 0 and above, with values closer to 0 indicating perfect prediction. Much like expressing RMSE as a percent, RSR is valuable to interpret RMSE in terms of the context of the variance in the observations. Values below 1 suggest that the model yields predictions less variable than the standard deviation, while values above 1 suggest that the prediction is imprecise.

On the other hand, Pearson's correlation coefficients (r) and the coefficient of determination (R$^2$) are two common linearity-based metrics:
\begin{equation} \label{eq_r}
    \begin{aligned}
    r &= \frac{\text{cov}(y, \hat{y})}{\sigma_y \sigma_{\hat{y}}} \\
    &= \frac{\sum_{i=1}^{n} (y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}})}{\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2 \sum_{i=1}^{n} (\hat{y}_i - \bar{\hat{y}})^2}}
    \end{aligned}
\end{equation}

\begin{equation} \label{eq_R2}
    \begin{aligned}
    R^2 &= 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}} \\
    &= 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
    \end{aligned}
\end{equation}

where \(SS_{\text{residual}}\) is the residual sum of squares and \(SS_{\text{total}}\) is the total sum of squares. Each \(y_i\) and \(\hat{y}_i\) are the ith elements of the actual response vector \(y\) and the predicted response vector \(\hat{y}\), respectively. \(\bar{y}\) and \(\bar{\hat{y}}\) are their respective means. Both \(r^2\) and \(R^2\) are scale invariant, meaning their values are unaffected by the scale of the observed data because they are normalized by the variation in the denominator.

The correlation coefficient \(r\) measures the strength of the linear relationship between two continuous variables, \(y\) and \(\hat{y}\), and ranges from -1 to 1. A value of 0 indicates no prediction accuracy in the evaluated model. One special characteristic of correlation \(r\) is that it is unaffected by the scale of the predictions or biases; it focuses on the relative changes in the predicted values compared to the true values. Thus, even if the prediction biases are scaled up or down, the correlation \(r\) between \(\hat{y}\) and \(y\) remains the same. This property is particularly useful when the focus is more on ranking predictions rather than their absolute values. For example, this metric has been used to evaluate models that identify high-performing production individuals, demonstrating the ability to predict nutrient digestibility in dairy cows \citep{de_souza_predicting_2018} and to select models based on their ability to rank traits such as feed intake and milk composition in dairy cows \citep{dorea_mining_2018,rovere_prediction_2021}.

The coefficient of determination \(R^2\) quantifies model performance from the proportion of variance in the dependent variable that is predictable from the independent variables. It ranges from negative infinity to 1, where 1 indicates that the model explains all the variance in the dependent variable, and 0 indicates that the model performs no better than predicting all samples as the mean of the observed values. \(R^2\) is useful in comparing multiple regression models, as demonstrated in studies that regress body weight of dairy cows on a set of morphological traits \citep{xavier_use_2022}, examine the relationship between milk spectral profiles and nitrogen utilization efficiency \citep{grelet_potential_2020}, and evaluate the predictive performance of milk fatty acid composition \citep{mantysaari_body_2019}.

It worth noting that many literatures have misinterpreted the relationship between $r$ and $R^2$. The coefficient of determination $R^2$ is not always equivalent to the square of the correlation coefficient $r^2$. The equivalence only holds when the same dataset is used for both model fitting and evaluation in a least squares regression model. The model assumes a zero covariance between the fitted residual and the predicted values $\hat{y}$, and it also assumes that the residuals (i.e., prediction biases) are centered on zero. In practice when predictions are made on new data, those assumptions are often violated, leading to discrepancies between $r^2$ and $R^2$. A details derivation of the equivalence is provided in Equation ~\ref{eq_pf_cov} ~\ref{eq_pf_r2} in the Appendix.


In addition to \(r^2\) and \(R^2\), another linearity-based metric is Lin's concordance correlation coefficient (CCC) \citep{lin_concordance_1989}:

\begin{equation} \label{eq_ccc}
\begin{aligned}
\text{CCC} &= \frac{2r \sigma_y \sigma_{\hat{y}}}{\sigma_y^2 + \sigma_{\hat{y}}^2 + (\bar{y} - \bar{\hat{y}})^2} \\
&= \frac{2 \text{cov}(y, \hat{y})}{\sigma_y^2 + \sigma_{\hat{y}}^2 + (\bar{y} - \bar{\hat{y}})^2}
\end{aligned}
\end{equation}

where $r$ is the Pearson correlation coefficient. The CCC is a comprehensive metric because it considers both the correlation and the scale bias between the predicted and true values. It fills the gap left by \(r^2\) where the scale bias is ignored. Geometrically, CCC measures how well the predicted values \(\hat{y}\) fall on the 45-degree line in a scatter plot of the predicted (x-axis) and true values (y-axis). It is advantageous over \(R^2\) because it consistently ranges from -1 to 1, making it easier to interpret and compare across different studies. The CCC is crucial when precise predictions are required for both the scale and the rank of the trait of interest, such as in studies predicting cotton crop yields based on soil and terrain profiles \citep{jones_identifying_2022}.

\subsubsection{Metrics in Classification Tasks}


\begin{table}
    \caption{Summary of model performance metrics for classification tasks.}
    \centering
    \begin{tabular}{llll}
        \toprule
        Metric & Label-invariant & Threshold-independent \\
        \midrule
        Accuracy & No & No \\
        Precision & No & No \\
        Recall & No & No \\
        F1 score & No & No \\
        Area under the precision-recall curve (AUC-PR) & No & Yes \\
        Area under the receiver operating characteristic curve (AUC-ROC) & Yes & Yes \\
        Matthews correlation coefficient (MCC) & Yes & Yes \\
        \bottomrule
    \end{tabular}
    \label{tab:metrics-cls}
\end{table}

Classification models aim to predict categorical outcomes such as 'healthy' or 'sick,' 'susceptible' or 'resistant,' and 'high yield' or 'low yield.’ To evaluate classification performance, one must first establish a confidence threshold to dichotomize the prediction probabilities. For instance, if a prediction probability exceeds the threshold, the sample is predicted as a positive sample. It is worth mentioning that this threshold is adjustable to fine-tune model performance for particular uses. The discussed metrics in classification tasks are summarized in Table~\ref{tab:metrics-cls}.

Accuracy is the most straightforward metric for evaluating classification models:

\begin{equation} \label{eq_accuracy}
    \begin{split}
\text{Accuracy} &= \frac{\text{Total Correct Predictions}}{\text{Total Predictions}} \\
        &= \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
    \end{split}
\end{equation}

where TP, TN, FP, and FN represent the number of true positives, true negatives, false positives, and false negatives, respectively. It summarizes an overall model performance by calculating the proportion of correctly classified samples among all samples. Nonetheless, accuracy can be misleading when the classes are imbalanced. For example, if a study predicting the presence of a specific event, of which the prevalence was only 10\%. In this case, a model that predicts all samples as negative would achieve an accuracy of 90\%, which is misleadingly high. To address this issue, precision and recall are introduced:

\begin{equation} \label{eq_precision}
    \begin{split}
    \text{Precision} &= \frac{\text{TP}}{\text{Total Predicted Positives}}\\
                    &=\frac{\text{TP}}{\text{TP} + \text{FP}}
    \end{split}
\end{equation}

\begin{equation} \label{eq_recall}
    \begin{split}
    \text{Recall} &= \frac{\text{TP}}{\text{Total Actual Positives}}\\
                &=\frac{\text{TP}}{\text{TP} + \text{FN}}
    \end{split}
\end{equation}

\begin{equation} \label{eq_f1}
    \text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

Precision and recall refine the assessment of a classification model by offering insights that accuracy alone may overlook. Precision calculates the fraction of true positives among all positive predictions, essentially measuring the trustworthiness of positive predictions made by the model (Eq.~\ref{eq_precision}). High precision is crucial in scenarios where false positives incur significant costs, and false negatives are more tolerable. For instance, in contexts where clinical treatments and culling are expensive, such as detecting bovine tuberculosis \citep{denholm_predicting_2020} or mastitis \citep{kandeel_ability_2019} using non-invasive methods, a high-precision model is crucial to minimize unnecessary costs and interventions from false positives. On the other hand, recall, also known as sensitivity, quantifies the ratio of true positives to all actual positives, assessing the model's ability to identify positive cases (Eq.~\ref{eq_recall}). High recall is essential where missing a positive case has serious consequences, or where false positives are easily rectifiable. For instance, detecting lameness or abnormal gait is crucial, as these can indicate underlying pathologies \citep{oleary_invited_2020} and impact welfare-related transport decisions \citep{stojkov_hot_2018}. An automated detection system \citep{oleary_invited_2020, alsaaod_automatic_2019,kang_accurate_2020} with high recall can mitigate economic losses by flagging at-risk cows. The benefit here lies in the feasibility of re-examining false positives, thus preventing more severe outcomes from undetected cases. Lastly, the F1 score, which is the harmonic mean of precision and recall, provides a balanced measure of model performance (Eq.~\ref{eq_f1}). It is usually used as an overall performance metric when precision and recall are equally important.

However, it is worth emphasizing that precision and recall focus predominantly on positive samples. Inappropriately assigning a predominant background event as the positive class can lead to skewed interpretations. Hence, the Receiver Operating Characteristic (ROC) curve provides an another crucial tool for assessing a model's performance in a label-agnostic manner, meaning it is not biased by the class distribution as precision and recall are. An ROC curve plots one minus specificity against sensitivity. The equations for specificity and sensitivity are as follows:

\begin{equation} \label{eq_specificity}
    \begin{split}
    \text{Specificity} &= \frac{\text{TN}}{\text{Total Actual Negatives}} \\
                        &=\frac{\text{TN}}{\text{FP} + \text{TN}}
    \end{split}
\end{equation}

\begin{equation} \label{eq_sensitivity}
    \begin{split}
    \text{Sensitivity} &= \text{Recall} \\
                        &= \frac{\text{TP}}{\text{Total Actual Positives}} \\
                        &=\frac{\text{TP}}{\text{TP} + \text{FN}}
    \end{split}
\end{equation}

A model's effectiveness, as depicted on the ROC curve, is gauged by how closely a point on the curve approaches the top-left corner. A steep ascent from the left side of the curve signifies the model's ability to correctly identify most true positives while incurring a low rate of false positives. A random guess, with a 50\% chance of correct prediction, corresponds to a diagonal line on the ROC curve. In dairy science, the ROC curve has been extensively utilized, for example, in predicting mastitis from milk composition \citep{jensen_bayesian_2016} and diagnosing pregnancy using spectroscopy technology \citep{delhez_diagnosing_2020}. In this hypothetical example, the ROC curve also demonstrates robustness and label-invariance with a consistent AUC of 0.875, regardless of whether the original or inverted labels are used.

In addtion to the metrics, the Matthews Correlation Coefficient (MCC) provides a symmetrical measure of the quality of binary classifications. The MCC considers both positive and negative samples in the dataset, providing a balanced measure of a model's performance \citep{chicco_advantages_2020}. It is defined as:

\begin{equation} \label{eq_mcc}
    \text{MCC} = \frac{\text{TP} \times \text{TN} - \text{FP} \times \text{FN}}{\sqrt{(\text{TP} + \text{FP})(\text{TP} + \text{FN})(\text{TN} + \text{FP})(\text{TN} + \text{FN})}}
\end{equation}

The equation ~\ref{eq_mcc} symmetrically incorporates all four components of TP, TN, FP, and FN). This symmetry makes MCC invariant to class distribution changes. The coefficient ranges from -1 to 1, where 1 indicates perfect classification, 0 indicates no better performance than random guessing, and -1 signifies total disagreement between prediction and observation. In a case study that used feeding and daily activity behaviors to diagnose Bovine Respiratory Disease in dairy calves, MCC proved particularly insightful \citep{bowen_early_2021}. The models in this study exhibited strong performance on negative samples (i.e., healthy calves), which were more prevalent, resulting in high specificity. However, sensitivity was relatively low at 0.54. In this context, MCC, with a value of 0.36, provided a more nuanced and representative measure of model performance, especially given the skew towards negative samples.

\subsection{Study Objectives}

This simulation study aims to highlight how biased or over-optimistic estimations of model performance usually come from inappropriately conducting CV, a technique crucial for characterizing expected model performance on “new” data. We demonstrate how common pitfalls, including using the exact data for both training and model assessment, excluding the model selection process from CV, and neglecting experimental block effects, contribute to challenges in model evaluation. Further, we scrutinize common metrics used in evaluating prediction models, including those used for regression and classification tasks. Because no single metric provides a comprehensive perspective of model performance, we seek, through this work, to highlight the importance of understanding the underlying theory of each metric to avoid misleading conclusions.

There are five simulation studies being conducted to address these challenges. The first simulation study will focus on the bias-variance trade-off in CV, demonstrating how the choice of K in K-fold CV affects the evaluation bias and variance. The second simulation study will investigate the impact of mistakenly using the same data for model selection and evaluation, highlighting the inflated model performance. The third simulation study will explore the effect of excluding block effects in CV, demonstrating how ignoring block effects can lead to over-optimistic model performance. The fourth simulation study will present four hypothetical predictions made in the same regression tasks, leading to different interpretations with different metrics. The fifth simulation study will demonstrate the impact of imbalanced data on classification model evaluation, showing how the choice of metrics can lead to misleading conclusions. Overall, this series of simulation studies aims to guide researchers in accurately and consistently reporting model performance, thereby supporting integrity and scientific rigor in prediction modeling research.
