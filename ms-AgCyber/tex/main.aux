\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Modeling}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Model Evaluation}{2}{subsection.1.2}\protected@file@percent }
\citation{cheng_efficient_2017}
\citation{van_dixhoorn_indicators_2018}
\citation{hastie_elements_2009,cawley_over-fitting_2010}
\citation{hoerl_ridge_1970}
\citation{tibshirani_regression_1996}
\citation{drucker_support_1996}
\citation{abdi_partial_2003}
\citation{breiman_random_2001}
\citation{lecun_generalization_1989}
\citation{ghaffari_metabolomics_2019}
\citation{hastie_elements_2009}
\citation{rovere_prediction_2021}
\citation{becker_predicting_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Model Selection}{4}{subsection.1.3}\protected@file@percent }
\citation{lahart_predicting_2019}
\citation{bresolin_infrared_2020}
\citation{grelet_potential_2020}
\citation{rovere_prediction_2021}
\citation{adriaens_productive_2020,mota_real-time_2022}
\citation{spoliansky_development_2016,yukun_automatic_2019}
\citation{song_automated_2018,xavier_use_2022}
\citation{rovere_prediction_2021,mota_real-time_2022,mantysaari_body_2019,frizzarin_predicting_2021}
\citation{grelet_potential_2020,appuhamy_prediction_2016,de_souza_predicting_2018}
\citation{van_dixhoorn_indicators_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Cross Validation Design with Block Effects}{5}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Model Performance Metrics}{5}{subsection.1.5}\protected@file@percent }
\citation{song_automated_2018,xavier_use_2022}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of model performance metrics for regression tasks.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:metrics-reg}{{1}{6}{Summary of model performance metrics for regression tasks}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Metrics in Regression Tasks}{6}{subsubsection.1.5.1}\protected@file@percent }
\newlabel{eq_rmse}{{1}{6}{Metrics in Regression Tasks}{equation.1.1}{}}
\newlabel{eq_mae}{{2}{6}{Metrics in Regression Tasks}{equation.1.2}{}}
\citation{de_souza_predicting_2018}
\citation{dorea_mining_2018,rovere_prediction_2021}
\citation{xavier_use_2022}
\citation{grelet_potential_2020}
\citation{mantysaari_body_2019}
\newlabel{eq_rmspe}{{3}{7}{Metrics in Regression Tasks}{equation.1.3}{}}
\newlabel{eq_rsr}{{4}{7}{Metrics in Regression Tasks}{equation.1.4}{}}
\newlabel{eq_r}{{5}{7}{Metrics in Regression Tasks}{equation.1.5}{}}
\newlabel{eq_R2}{{6}{7}{Metrics in Regression Tasks}{equation.1.6}{}}
\citation{lin_concordance_1989}
\citation{jones_identifying_2022}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of model performance metrics for classification tasks.}}{8}{table.2}\protected@file@percent }
\newlabel{tab:metrics-cls}{{2}{8}{Summary of model performance metrics for classification tasks}{table.2}{}}
\newlabel{eq_ccc}{{7}{8}{Metrics in Regression Tasks}{equation.1.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Metrics in Classification Tasks}{8}{subsubsection.1.5.2}\protected@file@percent }
\citation{denholm_predicting_2020}
\citation{kandeel_ability_2019}
\citation{oleary_invited_2020}
\citation{stojkov_hot_2018}
\citation{oleary_invited_2020,alsaaod_automatic_2019,kang_accurate_2020}
\newlabel{eq_accuracy}{{8}{9}{Metrics in Classification Tasks}{equation.1.8}{}}
\newlabel{eq_precision}{{9}{9}{Metrics in Classification Tasks}{equation.1.9}{}}
\newlabel{eq_recall}{{10}{9}{Metrics in Classification Tasks}{equation.1.10}{}}
\newlabel{eq_f1}{{11}{9}{Metrics in Classification Tasks}{equation.1.11}{}}
\citation{jensen_bayesian_2016}
\citation{delhez_diagnosing_2020}
\citation{chicco_advantages_2020}
\citation{bowen_early_2021}
\newlabel{eq_specificity}{{12}{10}{Metrics in Classification Tasks}{equation.1.12}{}}
\newlabel{eq_sensitivity}{{13}{10}{Metrics in Classification Tasks}{equation.1.13}{}}
\newlabel{eq_mcc}{{14}{10}{Metrics in Classification Tasks}{equation.1.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Study Objectives}{11}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Materials and Methods}{11}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Study 1: Evaluateion bias and variance of cross-validation}{11}{subsection.2.1}\protected@file@percent }
\newlabel{eq_kfoldcv}{{15}{12}{Study 1: Evaluateion bias and variance of cross-validation}{equation.2.15}{}}
\newlabel{eq_insample}{{16}{12}{Study 1: Evaluateion bias and variance of cross-validation}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Study 2: Model Selection in Cross-Validation}{13}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Workflow diagram illustrating four cross-validation strategies of feature selection (FS) and hyperparameter tuning (HT), where 0 denotes incorrect implementation and 1 indicates correct practice. $X'$ is the selected feature subset, $\mathbb  {E}[\hat  {g}(f_\mathcal  {D})]$ is the expected generalization performance, $f_\mathcal  {D}$ is the model trained on the training set without being revealed to the test set.}}{13}{figure.1}\protected@file@percent }
\newlabel{fig:s2_schemes}{{1}{13}{Workflow diagram illustrating four cross-validation strategies of feature selection (FS) and hyperparameter tuning (HT), where 0 denotes incorrect implementation and 1 indicates correct practice. $X'$ is the selected feature subset, $\mathbb {E}[\hat {g}(f_\mathcal {D})]$ is the expected generalization performance, $f_\mathcal {D}$ is the model trained on the training set without being revealed to the test set}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Study 3: Block Effects in Cross-Validation}{14}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of fold assignment in block cross validation (left) and random cross validation (right). Folds are color-coded, and the block effect is set to 3 in this example.}}{15}{figure.2}\protected@file@percent }
\newlabel{fig:s3_block}{{2}{15}{Illustration of fold assignment in block cross validation (left) and random cross validation (right). Folds are color-coded, and the block effect is set to 3 in this example}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Study 4: Performance Metrics in Regression Tasks}{15}{subsection.2.4}\protected@file@percent }
\citation{ouellet_evaluation_2016,borchers_machine-learning-based_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Study 5: Performance Metrics in Classification Tasks}{16}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results and Discussion}{17}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Study 1: The Impact of Estimator Choice and Sample Size on Model Evaluation Reliability}{17}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Simulation results of evaluation bias from 1000 sampling iterations. Multiple performance estimators across different sample sizes were color-coded. Three metrics: $r$, $R^2$, and RMSE, were displayed in the column facets.}}{18}{figure.3}\protected@file@percent }
\newlabel{fig:s1_bias}{{3}{18}{Simulation results of evaluation bias from 1000 sampling iterations. Multiple performance estimators across different sample sizes were color-coded. Three metrics: $r$, $R^2$, and RMSE, were displayed in the column facets}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Simulation results of evaluation bias and variance from 1000 sampling iterations. Multiple performance estimators across different sample sizes were color-coded. Only RMSE was displayed. Bias and variance were listed in the left and right facets, respectively.}}{18}{figure.4}\protected@file@percent }
\newlabel{fig:s1_var}{{4}{18}{Simulation results of evaluation bias and variance from 1000 sampling iterations. Multiple performance estimators across different sample sizes were color-coded. Only RMSE was displayed. Bias and variance were listed in the left and right facets, respectively}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Study 2: Misuse of Model Selection Can Lead to Over-Optimistic Performance Estimates}{19}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The evaluation bias of the four model selection strategies.}}{19}{figure.5}\protected@file@percent }
\newlabel{fig:s2_results}{{5}{19}{The evaluation bias of the four model selection strategies}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces ANOVA results for a single iteration of the simulated data with b = 0.5. SS: sum of squares; DF: degree of freedom; MS: mean square; F: F-statistic}}{20}{table.3}\protected@file@percent }
\newlabel{tab:anova}{{3}{20}{ANOVA results for a single iteration of the simulated data with b = 0.5. SS: sum of squares; DF: degree of freedom; MS: mean square; F: F-statistic}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Study 3: Overlooking Experimental Block Effects Can Lead to Biased Model Performance Estimates}{20}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Study 4: Different Regression Metrics Illustrate Different Aspects of Model Performance}{20}{subsection.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bias in model performance estimation by Block CV and Random CV across 1000 iterations. The dashed line represents the null hypothesis that the mean performance estimate is zero.}}{21}{figure.6}\protected@file@percent }
\newlabel{fig:s3_results}{{6}{21}{Bias in model performance estimation by Block CV and Random CV across 1000 iterations. The dashed line represents the null hypothesis that the mean performance estimate is zero}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Scatter plots display the same observations against four different prediction scenarios in the given hypothetical example. Scenario 1 serves as a baseline for the metrics, with any metric better than the baseline highlighted in bold and underscored, and any worse metric colored in red.}}{22}{figure.7}\protected@file@percent }
\newlabel{fig:s4_reg}{{7}{22}{Scatter plots display the same observations against four different prediction scenarios in the given hypothetical example. Scenario 1 serves as a baseline for the metrics, with any metric better than the baseline highlighted in bold and underscored, and any worse metric colored in red}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Simulated hypothetical example of binary classification. TP: true positive; FN: false negative; FP: false positive; TN: true negative; \textbf  {Upper}: The ground truth and prediction probability. \textbf  {Lower}: The confusion matrix of the prediction at a threshold of 0.5, followed by classification metrics of accuracy, precision, recall, MCC, PR curve AUC, and ROC curve AUC. The performance of the original labels serves as a baseline for comparison. Any better performance metrics from the inverted labels are highlighted in bold and underscored}}{23}{figure.8}\protected@file@percent }
\newlabel{fig:s5_cls}{{8}{23}{Simulated hypothetical example of binary classification. TP: true positive; FN: false negative; FP: false positive; TN: true negative; \textbf {Upper}: The ground truth and prediction probability. \textbf {Lower}: The confusion matrix of the prediction at a threshold of 0.5, followed by classification metrics of accuracy, precision, recall, MCC, PR curve AUC, and ROC curve AUC. The performance of the original labels serves as a baseline for comparison. Any better performance metrics from the inverted labels are highlighted in bold and underscored}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Study 5: Label-Invariant Metrics Provide Balanced Assessment in Binary Classification}{23}{subsection.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {(Left)} Precision-recall (PR) curve and \textbf  {(Right)} Receiver operating characteristic (ROC) curve for the hypothetical example are displayed. The performance at confidence thresholds of 0.25 and 0.50 is highlighted. Original labels are marked in green, while inverted labels appear in orange. The Area Under the Curve (AUC) is depicted at the center of each curve.}}{24}{figure.9}\protected@file@percent }
\newlabel{fig:s5_curve}{{9}{24}{\textbf {(Left)} Precision-recall (PR) curve and \textbf {(Right)} Receiver operating characteristic (ROC) curve for the hypothetical example are displayed. The performance at confidence thresholds of 0.25 and 0.50 is highlighted. Original labels are marked in green, while inverted labels appear in orange. The Area Under the Curve (AUC) is depicted at the center of each curve}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{25}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Matthews Correlation Coefficient (MCC) curve. A line chart plotting MCC at different thresholds for the hypothetical example. The optimal threshold is highlighted by the dot marks in green and orange for the original and inverted labels, respectively. The confusion matrix at the optimal threshold is displayed in the right panel.}}{26}{figure.10}\protected@file@percent }
\newlabel{fig:s5_mcc}{{10}{26}{Matthews Correlation Coefficient (MCC) curve. A line chart plotting MCC at different thresholds for the hypothetical example. The optimal threshold is highlighted by the dot marks in green and orange for the original and inverted labels, respectively. The confusion matrix at the optimal threshold is displayed in the right panel}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Acknowledgement}{26}{section.5}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{references}
\bibcite{cheng_efficient_2017}{{1}{}{{}}{{}}}
\bibcite{van_dixhoorn_indicators_2018}{{2}{}{{}}{{}}}
\bibcite{hastie_elements_2009}{{3}{}{{}}{{}}}
\bibcite{cawley_over-fitting_2010}{{4}{}{{}}{{}}}
\bibcite{hoerl_ridge_1970}{{5}{}{{}}{{}}}
\bibcite{tibshirani_regression_1996}{{6}{}{{}}{{}}}
\bibcite{drucker_support_1996}{{7}{}{{}}{{}}}
\bibcite{abdi_partial_2003}{{8}{}{{}}{{}}}
\bibcite{breiman_random_2001}{{9}{}{{}}{{}}}
\bibcite{lecun_generalization_1989}{{10}{}{{}}{{}}}
\bibcite{ghaffari_metabolomics_2019}{{11}{}{{}}{{}}}
\bibcite{rovere_prediction_2021}{{12}{}{{}}{{}}}
\bibcite{becker_predicting_2021}{{13}{}{{}}{{}}}
\bibcite{lahart_predicting_2019}{{14}{}{{}}{{}}}
\bibcite{bresolin_infrared_2020}{{15}{}{{}}{{}}}
\bibcite{grelet_potential_2020}{{16}{}{{}}{{}}}
\bibcite{adriaens_productive_2020}{{17}{}{{}}{{}}}
\bibcite{mota_real-time_2022}{{18}{}{{}}{{}}}
\bibcite{spoliansky_development_2016}{{19}{}{{}}{{}}}
\bibcite{yukun_automatic_2019}{{20}{}{{}}{{}}}
\bibcite{song_automated_2018}{{21}{}{{}}{{}}}
\bibcite{xavier_use_2022}{{22}{}{{}}{{}}}
\bibcite{mantysaari_body_2019}{{23}{}{{}}{{}}}
\bibcite{frizzarin_predicting_2021}{{24}{}{{}}{{}}}
\bibcite{appuhamy_prediction_2016}{{25}{}{{}}{{}}}
\bibcite{de_souza_predicting_2018}{{26}{}{{}}{{}}}
\bibcite{dorea_mining_2018}{{27}{}{{}}{{}}}
\bibcite{lin_concordance_1989}{{28}{}{{}}{{}}}
\bibcite{jones_identifying_2022}{{29}{}{{}}{{}}}
\bibcite{denholm_predicting_2020}{{30}{}{{}}{{}}}
\bibcite{kandeel_ability_2019}{{31}{}{{}}{{}}}
\bibcite{oleary_invited_2020}{{32}{}{{}}{{}}}
\bibcite{stojkov_hot_2018}{{33}{}{{}}{{}}}
\bibcite{alsaaod_automatic_2019}{{34}{}{{}}{{}}}
\bibcite{kang_accurate_2020}{{35}{}{{}}{{}}}
\bibcite{jensen_bayesian_2016}{{36}{}{{}}{{}}}
\bibcite{delhez_diagnosing_2020}{{37}{}{{}}{{}}}
\bibcite{chicco_advantages_2020}{{38}{}{{}}{{}}}
\bibcite{bowen_early_2021}{{39}{}{{}}{{}}}
\bibcite{ouellet_evaluation_2016}{{40}{}{{}}{{}}}
\bibcite{borchers_machine-learning-based_2017}{{41}{}{{}}{{}}}
\newlabel{eq_datasplit}{{17}{31}{Cross Validation}{equation.5.17}{}}
\newlabel{eq_traintest}{{18}{31}{Cross Validation}{equation.5.18}{}}
\newlabel{eq_g_est}{{19}{31}{Cross Validation}{equation.5.19}{}}
\newlabel{eq_g_exp}{{20}{32}{Cross Validation}{equation.5.20}{}}
\newlabel{eq_bias}{{21}{32}{Cross Validation Bias and Variance}{equation.5.21}{}}
\newlabel{eq_var}{{22}{32}{Cross Validation Bias and Variance}{equation.5.22}{}}
\newlabel{eq_tradeoff}{{23}{33}{Cross Validation Bias and Variance}{equation.5.23}{}}
\newlabel{eq_ols}{{24}{33}{Hyperparameter}{equation.5.24}{}}
\newlabel{eq_ridge}{{25}{33}{Hyperparameter}{equation.5.25}{}}
\newlabel{eq_lasso}{{26}{33}{Hyperparameter}{equation.5.26}{}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{eq_pf_cov}{{27}{34}{Sqaured Correlation Coefficient $r^2$ and Determination Coefficient $R^2$}{equation.5.27}{}}
\newlabel{eq_pf_r2}{{28}{34}{Sqaured Correlation Coefficient $r^2$ and Determination Coefficient $R^2$}{equation.5.28}{}}
\gdef \@abspage@last{34}
