# Introduction

Modeling is an essential tool for hypothesis formulation and decision-making. It functions as a structured framework that validates system understanding through the analysis of empirical data, and further extends this understanding by enabling the extrapolation of results to novel trials and conditions. The advancement of research is fundamentally built upon the accumulation of prior knowledge within the scientific community. Therefore, the evaluation of model performance becomes particularly critical, necessitating a rigorous and standardized approach that allows for both reproducibility and comparability. The failure to adhere to these standards, by reporting model performance through ill-defined metrics or non-rigorous procedures, can introduce misinterpretations and miscommunications. Such lapses not only impede scientific progress but can also compromise the integrity of the collective body of research in the field.
This review aims to scrutinize commonly used metric in evaluating a prediction model, covering both tasks of regression and classification. Since there is no single metric providing a comprehensive perspective of model performance, understanding the underlying theory of each metric helps avoid misleading conclusions. Additionally, biased, or over-optimistic, estimations of model performance usually come from inappropriately conducting cross validation (CV), which is a technique to simulate unseen data for model evaluation. Common pitfalls include using the same data for both model training and evaluation, excluding model selection process from CV, and neglecting experimental block effects. This review uses a series of hypothetical examples to demonstrate these pitfalls and is anticipated to further address concerns in model evaluation, such as how to report model performance when the dataset is imbalanced in binary classification? How to determine the number of folds in CV? How to unbiasedly estimate a model performance with limited samples?
This review is organized into two key sections that explore the various facets of model performance metric and validation in depth. In addition to the brief overview in this first section, section 2 delvse into the metrics used for assessing model performance, with subsection 2.a focusing on regression metrics and subsection 2.b examining classification metrics. Section 3 is dedicated to the validation techniques essential for ensuring model reliability and includes discussions on bias and variance (subsection 3.a), model selection methodologies (subsection 3.b), and the application of block cross-validation strategies (subsection 3.c). Each subsection is accompanied by a hypothetical example or simulation to illustrate the concepts from both theoretical and practical perspectives. Finally, section 4 provides a summary of the key points and recommendations for future research.
