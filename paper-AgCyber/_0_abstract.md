# Summary

## Interpretive Summary

This study provides practical guidelines to support evaluating prediction models used in precision agricultural contexts. It emphasizes the need for a deep understanding of various performance metrics, highlighting their applications and limitations. Additionally, the paper addresses common pitfalls in cross-validation, a key method for model assessment. By offering computational simulations and practical examples, this study is intended to support the decision-making process of researchers developing prediction models, supporting transparent and accurate reporting of model performance. The guidelines are intended to support integrity and scientific rigor in prediction modeling research.

## Abstract

This study critically examines the metrics and methodologies used for evaluating prediction models in regression and classification tasks, making a case for the application of rigorous and standardized approaches in model performance assessment. Within the context of this work, we define modeling as a structured framework for hypothesis formulation and decision-making, which relies on the analysis and extrapolation of empirical data. The advancement of modeling is contingent on the accumulation of prior knowledge within the scientific community.
The study highlights that no single model performance metric suffices to adequately and conservatively represent model performance, emphasizing the need for understanding the underlying theory of each metric to avoid misleading conclusions. In regression tasks, metrics such as the Pearson Correlation Coefficient, Root Mean Squared Error, and Coefficient of Determination R^2 are discussed  , considering their specific applications and limitations. For classification tasks, the focus shifts to metrics including precision, recall, ROC curve, and MCC curve, emphasizing correctly designating the positive class to prevent bias and ensuring label-invariant assessment for a balanced evaluation. Moreover, the paper delves into common pitfalls in cross-validation (CV), a technique crucial for characterizing expected model performance on “new” data.  Issues such as using the same data for both training and assessment, excluding model selection from CV, and overlooking experimental block effects are explored.
The study is structured into sections covering model performance metrics and validation techniques. Each section provides computational simulations and literature examples to illustrate the concepts from both theoretical and practical perspectives, culminating in a summary of key points and recommendations for future research. This comprehensive approach aims to guide researchers in accurately and consistently reporting model performance, especially in scenarios containing imbalanced datasets, feature selection, hyperparameter tuning, and limited sample sizes.

Key Words: cross-validation; model evaluation; simulation
