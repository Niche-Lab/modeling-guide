# Introduction

## Study overview

Modeling is an essential tool for hypothesis formulation and decision-making. It functions as a structured investigatory framework that allows researchers to explore system understanding through the summary and analysis of empirical data. Carefully constructed and evaluated models offer the potential to extend this understanding by enabling the extrapolation of results to novel trials and conditions. Although only one focus of the science of modeling, the predictive role is often explicitly or implicitly the ultimate goal of models derived within the precision agriculture context. Through this lens, modeling provides opportunity to standardize and formalize research advancement, through developing quantitative constructs that accumulate prior knowledge derived by the broader the scientific community. Evaluating model performance becomes particularly critical when considering this role within the knowledge generation enterprise, necessitating a rigorous and standardized approach that allows for both reproducibility and comparability. As more and more model-based exercises are developed using slightly different methods, or slightly different datasets, it becomes increasingly challenging to evaluate, characterize, compare, and balance information generated by the resulting modeling tools, particularly when results are conflicting. Specifically, reporting model performance through poorly-defined metrics or incomplete procedures can create opportunity for confusion, misinterpretation, and miscommunication, while can ultimately result in distrust in model-based tools and impede scientific progress.
This review aims to scrutinize common metrics used in evaluating prediction models, including those used for regression and classification tasks. Because no single metric provides a comprehensive perspective of model performance, we seek, through this work, to highlight the importance of understanding the underlying theory of each metric to avoid misleading conclusions. Further, we highlight how biased or over-optimistic estimations of model performance usually come from inappropriately conducting cross-validation (CV), a technique to simulate unseen data for model evaluation. Using a simulation approach, we demonstrate how common pitfalls, including using the exact data for both training and model assessment, excluding the model selection process from CV, and neglecting experimental block effects, contribute to challenges in model evaluation. Through these examples, we raise a number of additional questions to support guideliens for model evaluation, including:1) how to appropriately report model performance when the dataset is imbalanced in binary classification; 2)How to include feature selection and hyperparameter tuning in a CV; and 3) How to estimate model performance from limited samples in a minimally-biased manner?
This review is organized into two key sections that explore the various facets of model performance metrics in depth. In addition to the brief overview in this first section, section 2 delves into the metrics used for assessing model performance, with subsection 2. a focusing on regression metrics and subsection 2. b examining classification metrics. Section 3 is dedicated to  evalution techniques essential for ensuring model reliability, including discussions on bias and variance (subsection 3. a), model selection methodologies (subsection 3. b), and the application of block cross-validation strategies (subsection 3. c). To support these more theoretical discussions, an example computational simulation was conducted to provide a hypothetical showcase of points raised in each subsection, accompanied by several examples from the literature as real-world  illustrations of theoretical and practical concepts. Lastly, Section 4 summarizes key points and recommendations for future research.
