\subsection{Simulation experiments to Understand Model Evaluation Pitfalls}

In this study, there are five simulation experiments being conducted to address common challenges in model evaluation. The first simulation experiment will focus on the bias-variance trade-off in CV, demonstrating how the choice of K in K-fold CV affects the evaluation bias and variance. The second simulation experiment will investigate the impact of mistakenly using the same data for model selection and evaluation, highlighting the inflated model performance. The third simulation experiment will explore the effect of excluding block effects in CV, demonstrating how ignoring block effects can lead to over-optimistic model performance. The fourth simulation experiment will explore how various metrics respond to different combinations of bias and variance in prediction errors, illustrating how these variations can lead to distinct interpretations of model performance. The fifth simulation experiment will examine the impact of imbalanced data on classification model evaluation, highlighting how the choice of metrics can influence conclusions and potentially lead to misleading interpretations. Together, this series of simulation studies aims to provide guidance for researchers on accurately and consistently reporting model performance, thereby promoting integrity and scientific rigor in prediction modeling research.