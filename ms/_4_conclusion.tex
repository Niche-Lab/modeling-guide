\section{Conclusion}

In conclusion, this study presents a comprehensive evaluation of five simulation experiments, uncovering critical insights into the interplay of performance estimators, metrics, and contextual factors that influence model evaluation reliability. The findings highlight the nuanced impact of estimator choices and sample sizes on bias and variance, emphasizing that while traditional estimators such as LOOCV can be reliable for error-based metrics, they may severely underestimate correlation-based metrics under certain conditions. The misuse of model selection processes was shown to substantially inflate performance estimates, reinforcing the importance of adhering to rigorous cross-validation practices. Additionally, the role of experimental block effects in biasing performance estimates underscores the necessity of aligning evaluation strategies with real-world applications. In both regression and classification tasks, metric characteristics vary significantly, with different metrics offering complementary perspectives on model performance. For regression tasks, CCC and $R^2$ provided interpretable benchmarks for understanding prediction errors, while MAE demonstrated greater robustness to variance compared to RMSE. In classification tasks, metrics such as precision, F1 score, and MCC were shown to capture different facets of performance, with MCC offering a balanced, stringent evaluation across multiple dimensions. Collectively, these findings stress the importance of tailoring model evaluation strategies to specific research and application contexts to ensure robust, reliable, and interpretable results.

\section{Code Availability}

All the implementation codes for the simulation experiments and data analysis are available at \url{https://github.com/Niche-Lab/modeling-guide/}.

\section{Acknowledgement}

The author James Chen expresses his gratitude to Drs. Zhiwu Zhang, Hao Cheng, Gota Morota, and Gonzalo Ferreira for their insightful discussions that partially contributed to this study. The authors declare no conflicts of interest.

\section*{Declaration of generative AI and AI-assisted technologies in the writing process}

During the preparation of this work the author(s) used ChatGPT in order to correct grammar and improve the readability of the manuscript. The author(s) reviewed and edited the content as needed after using this tool/service and take(s) full responsibility for the content of the publication.